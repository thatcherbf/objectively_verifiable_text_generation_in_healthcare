{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ni3mpyl7jo6a"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "d-OQngtx4CQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio --quiet"
      ],
      "metadata": {
        "id": "IMXK2n0r8vsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers --quiet"
      ],
      "metadata": {
        "id": "ccA8yGo682Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUzELRkZIKUL"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate --upgrade --quiet"
      ],
      "metadata": {
        "id": "9U5iFt9BIQ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "VElB7oFJJ_zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF pdfminer.six --quiet"
      ],
      "metadata": {
        "id": "kWwDVYhLIYli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --quiet"
      ],
      "metadata": {
        "id": "1Y-Iyo0DIbn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl --quiet"
      ],
      "metadata": {
        "id": "_2ZE8eSxQdM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5gedqt4l4FXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "FO9kSxMiI3Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "HGtuZB0skiQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "IZ47-zuMj3Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
        "from pdfminer.high_level import extract_text\n",
        "import glob"
      ],
      "metadata": {
        "id": "z5fBrakflAuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc"
      ],
      "metadata": {
        "id": "68idbHAQI66f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "vryjYAZfj0u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "4EUZ37vYlX62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace/Drive integration"
      ],
      "metadata": {
        "id": "fx9lfYVO4Nj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "Z3yVsnXRMeT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pPVZAlZaIoZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "iRQJgOXN4QKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "TNLxUdu8Iky0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Free GPU memory"
      ],
      "metadata": {
        "id": "ks79UzLl4RqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_memory():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "2u9euLsD9gnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load LLM"
      ],
      "metadata": {
        "id": "vKIQN5WV4TVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "auth_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "oHACt7s6JvP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token = auth_token,\n",
        "    cache_dir = '/content/drive/MyDrive/model',\n",
        ")"
      ],
      "metadata": {
        "id": "17Z0BFgRKRh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XchDlSN8KSVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token = auth_token,\n",
        "    cache_dir = '/content/drive/MyDrive/model',\n",
        "    torch_dtype = torch.float16,\n",
        "    rope_scaling = {\"type\": \"dynamic\", \"factor\": 2},\n",
        "    low_cpu_mem_usage = True,\n",
        "    device_map = \"auto\",\n",
        "    quantization_config = quantization_config\n",
        ")"
      ],
      "metadata": {
        "id": "Of_DN4PVLIo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1 — PEFT"
      ],
      "metadata": {
        "id": "8sOvT0Jo4XRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdfs(pdf_paths):\n",
        "    texts = []\n",
        "    for path in pdf_paths:\n",
        "        text = extract_text(path)\n",
        "        texts.append(text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "mAjPgvEEJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/data\""
      ],
      "metadata": {
        "id": "xm28OkPJJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = glob.glob(pdf_path+\"/*.pdf\")"
      ],
      "metadata": {
        "id": "mQuq6l08Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = extract_text_from_pdfs(pdf_paths)"
      ],
      "metadata": {
        "id": "d6Pe3Vb6Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove header/footer artifacts\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple whitespaces with single space\n",
        "    text = re.sub(r'(\\n){2,}', '\\n', text)  # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
        "\n",
        "    # Remove common but unnecessary items like references or excess newlines\n",
        "    text = text.replace('\\n', ' ')  # Replace new lines with space to maintain continuity\n",
        "    return text"
      ],
      "metadata": {
        "id": "F57GelOmJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "afIzXLmeJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size = 512, overlap = 50):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk = tokens[i:i + chunk_size]\n",
        "        chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ho8tgoeSJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenize function\n",
        "def tokenize_function(examples):\n",
        "    all_chunks = []\n",
        "    for example in examples['text']:\n",
        "        chunks = chunk_text(example)\n",
        "        for chunk in chunks:\n",
        "            tokenized_chunk = tokenizer(chunk, padding = \"max_length\", truncation = True, max_length = 512)\n",
        "            all_chunks.append(tokenized_chunk)\n",
        "\n",
        "    # Transform list of tokenized chunks into a dictionary of lists\n",
        "    batch = {key: [] for key in all_chunks[0].keys()}\n",
        "    for chunk in all_chunks:\n",
        "        for key, value in chunk.items():\n",
        "            batch[key].append(value)\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "9gTd1SEWJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the extracted texts\n",
        "texts_dataset = Dataset.from_dict({\"text\": texts})\n",
        "tokenized_dataset = texts_dataset.map(tokenize_function, batched = True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "UrD84o0CJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels (in causal language modeling, labels are the same as input_ids)\n",
        "def add_labels(example):\n",
        "    example['labels'] = example['input_ids'].copy()\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=False)"
      ],
      "metadata": {
        "id": "hrBSJRjFjVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 fine tuning"
      ],
      "metadata": {
        "id": "3OEVuseHbV51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    cls = bitsandbytes.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "7kKIVFVlM34q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "id": "DdsNP3a1M9lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LoRA configuration\n",
        "lora_config_phase1 = LoraConfig(\n",
        "    r = 16,  # rank of the low-rank approximation\n",
        "    lora_alpha = 64,  # scaling factor\n",
        "    target_modules = modules,  # target specific modules\n",
        "    lora_dropout = 0.1,  # dropout rate\n",
        "    bias = \"none\",  # whether to train biases\n",
        "    task_type = \"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "3yfTeIRSLaN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "Is70c-0i75Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "NJG6ZuWoPbZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config_phase1)"
      ],
      "metadata": {
        "id": "3FhaOWFX3szL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 2,\n",
        "    max_steps = 15,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = True,\n",
        "    logging_steps = 1,\n",
        "    output_dir = \"outputs\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        ")"
      ],
      "metadata": {
        "id": "Oig7kzU0MjxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "SHLTRd4R9Fvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "IFFcMh0r_70N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "id": "Tx0qDgKYNDjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "54xcB0PqQWNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/saved_models/phase1'\n",
        "trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "zTm_lqCPQWLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6cUtFoKTQWJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "dKThePAYQWHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_merged_dir = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1\"\n",
        "model.save_pretrained(\n",
        "    output_merged_dir,\n",
        "    safe_serialization = True\n",
        ")"
      ],
      "metadata": {
        "id": "dBDloinyRLPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "aZyGof3KRWAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(',\\n'.join(os.listdir(\"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1\")))"
      ],
      "metadata": {
        "id": "uZ0WluUZVqdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 Testing"
      ],
      "metadata": {
        "id": "bJiGRfvZR1Tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68o7PJWvTwiG"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the responses of the chatbot\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information to the user's queries.\n",
        "Please ensure that your responses are succinct, respectful, and factual. Refrain from emoting.\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "Respond to the User's question ONLY. Do not impersonate the User and do not include followup questions in your response unless prompted.\"\"\""
      ],
      "metadata": {
        "id": "VqiE1U5be8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the concept of plaque-years in the context of LDL cholesterol and cardiovascular health?\"\n",
        "\n",
        "prompt_with_system_prompt = f\"{system_prompt}\\nUser: {prompt} Assistant: \"  # Add the system prompt to the beginning of the conversation\n",
        "\n",
        "inputs = tokenizer(prompt_with_system_prompt, return_tensors = \"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    #max_length = 150,\n",
        "    temperature = 0.5,\n",
        "    top_p = 0.75\n",
        ")"
      ],
      "metadata": {
        "id": "ZFx43fQ8uoG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "display(Markdown(f\"\\n'''\\n{response_text.split('Assistant: ')[-1].strip()}\\n'''\\n\"))"
      ],
      "metadata": {
        "id": "TdgctgtVoL91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2 — Instruction FT"
      ],
      "metadata": {
        "id": "Ni3mpyl7jo6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phase2_qs = pd.read_csv('/content/drive/MyDrive/data/csv/phase2_questions_filled.csv')"
      ],
      "metadata": {
        "id": "olDQGwquj7yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phase2_qs_ds = Dataset.from_pandas(phase2_qs)"
      ],
      "metadata": {
        "id": "3xhFEEYlkL_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of prompts: {len(phase2_qs_ds)}')\n",
        "print(f'Column names are: {phase2_qs_ds.column_names}')"
      ],
      "metadata": {
        "id": "kuBA90GhkL9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nb_samples = 3\n",
        "random_indices = random.sample(range(len(phase2_qs_ds)), nb_samples)\n",
        "samples = []\n",
        "\n",
        "for idx in random_indices:\n",
        "    sample = phase2_qs_ds[idx]\n",
        "\n",
        "    sample_data = {\n",
        "        'instruction': sample['instruction'],\n",
        "        'response': sample['response'],\n",
        "    }\n",
        "\n",
        "    samples.append(sample_data)"
      ],
      "metadata": {
        "id": "4eUCiH-TlWs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(samples)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "V_qtHjxckAE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    instruction = f\"User: {sample['instruction']}\"\n",
        "    response = f\"Assistant: {sample['response']}\"\n",
        "\n",
        "    parts = [part for part in [instruction, response] if part]\n",
        "\n",
        "    formatted_prompt = \"\\n\".join(parts)\n",
        "\n",
        "    sample[\"text\"] = formatted_prompt\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "77yJ8jFzkL7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAubK-BuMgjd"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "model_directory = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_directory,\n",
        "    local_files_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "fsjYm73Wx5r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "qRPKQ38Wrae_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf7TT2EMiAr"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model from phase1\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_directory,\n",
        "    torch_dtype = torch.float16,\n",
        "    low_cpu_mem_usage = True,\n",
        "    rope_scaling = {\"type\": \"dynamic\", \"factor\": 2},\n",
        "    local_files_only = True,\n",
        "    quantization_config = quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def get_max_length(model):\n",
        "    conf = model.config\n",
        "    max_length = None\n",
        "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
        "        max_length = getattr(model.config, length_setting, None)\n",
        "        if max_length:\n",
        "            print(f\"Found max lenth: {max_length}\")\n",
        "            break\n",
        "    if not max_length:\n",
        "        max_length = 1024\n",
        "        print(f\"Using default max length: {max_length}\")\n",
        "\n",
        "    return max_length\n",
        "\n",
        "\n",
        "def preprocess_batch(batch, tokenizer, max_length):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        max_length = max_length,\n",
        "        truncation = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "DleBShV_lyQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
        "def preprocess_dataset(tokenizer, max_length, dataset):\n",
        "    # Add prompt to each sample\n",
        "    print(\"Preprocessing dataset...\")\n",
        "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
        "\n",
        "    # Apply preprocessing to each batch of the dataset & and remove 'instruction', 'context', 'response', 'category' fields\n",
        "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
        "    dataset = dataset.map(\n",
        "        _preprocessing_function,\n",
        "        batched = True,\n",
        "        remove_columns = [\"instruction\", \"response\", \"text\"],\n",
        "    )\n",
        "\n",
        "    # Filter out samples that have input_ids exceeding max_length\n",
        "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    dataset = dataset.shuffle()\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "rHaPzegdlyOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = get_max_length(model)\n",
        "phase2_qs_ds = preprocess_dataset(tokenizer, max_length, phase2_qs_ds)"
      ],
      "metadata": {
        "id": "m8dIBamHlyMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    cls = bitsandbytes.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "LgwrAhNPrDqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "id": "OokgMcWgoskw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LoRA configuration\n",
        "lora_config_phase2 = LoraConfig(\n",
        "    r = 4,  # rank of the low-rank approximation - lower for phase 2\n",
        "    lora_alpha = 16,  # scaling factor\n",
        "    target_modules = modules,  # target specific modules\n",
        "    lora_dropout = 0.1,  # dropout rate\n",
        "    bias = \"none\",  # whether to train biases\n",
        "    task_type = \"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "vO0eBDVzosk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "oxP5Oo2posk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "9dp34O1tosk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config_phase2)"
      ],
      "metadata": {
        "id": "axUGAuuWosk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    warmup_steps = 2,\n",
        "    max_steps = 10,\n",
        "    learning_rate = 1e-4,\n",
        "    fp16 = True,\n",
        "    logging_steps = 1,\n",
        "    output_dir = \"outputs\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        ")"
      ],
      "metadata": {
        "id": "Ac9gIW5losk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = phase2_qs_ds,\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "e1t164lFosk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "tV9GwLezosk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "id": "r4bp_Q5Qosk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "KTu_V4vDosk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/saved_models/phase2'\n",
        "trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "RBznyvLlosk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "iMiIH86Qosk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "14vElymVosk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_merged_dir = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT2-v2\"\n",
        "model.save_pretrained(\n",
        "    output_merged_dir,\n",
        "    safe_serialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "3ScXh0l-osk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "DUgGZ4Syosk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(',\\n'.join(os.listdir(\"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT2-v2\")))"
      ],
      "metadata": {
        "id": "WRjfvMarosk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2 Testing"
      ],
      "metadata": {
        "id": "nSfFFlCisWy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX4oDw8jsdG7"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the responses of the chatbot\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information to the user's queries.\n",
        "Please ensure that your responses are succinct, respectful, and factual. Refrain from emoting.\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "Respond to the User's question ONLY. Do not impersonate the User and do not include followup questions in your response unless prompted.\"\"\""
      ],
      "metadata": {
        "id": "aoNgxLIVsdG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the concept of plaque-years in the context of LDL cholesterol and cardiovascular health?\"\n",
        "\n",
        "prompt_with_system_prompt = f\"{system_prompt}\\nUser: {prompt}\\nAssistant: \"\n",
        "\n",
        "inputs = tokenizer(prompt_with_system_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids = inputs[\"input_ids\"].to(device),\n",
        "    attention_mask = inputs[\"attention_mask\"],\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    # max_new_tokens = 512,\n",
        "    temperature = 0.5,\n",
        "    top_p = 0.75\n",
        ")"
      ],
      "metadata": {
        "id": "sTnjcxMWsdG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "display(Markdown(f\"\\n'''\\n{response_text.split('Assistant: ')[-1].strip()}\\n'''\\n\"))"
      ],
      "metadata": {
        "id": "DZhqYZUDo3KX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}