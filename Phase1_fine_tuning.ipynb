{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "d-OQngtx4CQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "VElB7oFJJ_zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF pdfminer.six --quiet"
      ],
      "metadata": {
        "id": "kWwDVYhLIYli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl --quiet"
      ],
      "metadata": {
        "id": "_2ZE8eSxQdM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5gedqt4l4FXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "5wrVWQAmVPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "7a1og9jipnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1"
      ],
      "metadata": {
        "id": "8sOvT0Jo4XRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text"
      ],
      "metadata": {
        "id": "U_4EqQVlvPZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdfs(pdf_paths):\n",
        "    texts = []\n",
        "    for path in pdf_paths:\n",
        "        text = extract_text(path)\n",
        "        texts.append(text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "mAjPgvEEJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "pdf_path = \"/data\""
      ],
      "metadata": {
        "id": "xm28OkPJJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = glob.glob(pdf_path+\"/*.pdf\")"
      ],
      "metadata": {
        "id": "mQuq6l08Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = extract_text_from_pdfs(pdf_paths)"
      ],
      "metadata": {
        "id": "d6Pe3Vb6Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove header/footer artifacts\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple whitespaces with single space\n",
        "    text = re.sub(r'(\\n){2,}', '\\n', text)  # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
        "\n",
        "    # Remove common but unnecessary items like references or excess newlines\n",
        "    text = text.replace('\\n', ' ')  # Replace new lines with space to maintain continuity\n",
        "    return text"
      ],
      "metadata": {
        "id": "F57GelOmJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "afIzXLmeJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size = 512, overlap = 50):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  chunks = []\n",
        "  for i in range(0, len(tokens), chunk_size - overlap):\n",
        "    chunk = tokens[i:i + chunk_size]\n",
        "    chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "ho8tgoeSJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenize function\n",
        "def tokenize_function(examples):\n",
        "  all_chunks = []\n",
        "  for example in examples['text']:\n",
        "    chunks = chunk_text(example)\n",
        "    for chunk in chunks:\n",
        "      tokenized_chunk = tokenizer(chunk, padding=\"max_length\", truncation=True, max_length=512)\n",
        "      all_chunks.append(tokenized_chunk)\n",
        "\n",
        "  # Transform list of tokenized chunks into a dictionary of lists\n",
        "  batch = {key: [] for key in all_chunks[0].keys()}\n",
        "  for chunk in all_chunks:\n",
        "    for key, value in chunk.items():\n",
        "      batch[key].append(value)\n",
        "  return batch"
      ],
      "metadata": {
        "id": "9gTd1SEWJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the extracted texts\n",
        "texts_dataset = Dataset.from_dict({\"text\": texts})\n",
        "tokenized_dataset = texts_dataset.map(tokenize_function, batched = True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "UrD84o0CJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels (in causal language modeling, labels are the same as input_ids)\n",
        "def add_labels(example):\n",
        "    example['labels'] = example['input_ids'].copy()\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=False)"
      ],
      "metadata": {
        "id": "hrBSJRjFjVHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}