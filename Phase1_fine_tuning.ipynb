{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "d-OQngtx4CQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio --quiet"
      ],
      "metadata": {
        "id": "IMXK2n0r8vsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers --quiet"
      ],
      "metadata": {
        "id": "ccA8yGo682Cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUzELRkZIKUL"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate --upgrade --quiet"
      ],
      "metadata": {
        "id": "9U5iFt9BIQ9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "VElB7oFJJ_zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF pdfminer.six --quiet"
      ],
      "metadata": {
        "id": "kWwDVYhLIYli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft --quiet"
      ],
      "metadata": {
        "id": "1Y-Iyo0DIbn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl --quiet"
      ],
      "metadata": {
        "id": "_2ZE8eSxQdM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "5gedqt4l4FXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ],
      "metadata": {
        "id": "FO9kSxMiI3Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "68idbHAQI66f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc"
      ],
      "metadata": {
        "id": "ZFUVC0AY9d-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "5wrVWQAmVPCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login"
      ],
      "metadata": {
        "id": "oUfcnU8kMcBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "7a1og9jipnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace/Drive integration"
      ],
      "metadata": {
        "id": "fx9lfYVO4Nj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "Z3yVsnXRMeT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive, userdata\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pPVZAlZaIoZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "iRQJgOXN4QKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "TNLxUdu8Iky0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Free GPU memory"
      ],
      "metadata": {
        "id": "ks79UzLl4RqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_memory():\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "2u9euLsD9gnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load LLM"
      ],
      "metadata": {
        "id": "vKIQN5WV4TVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "uncFKFZuJkHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "auth_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "oHACt7s6JvP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    token=auth_token,\n",
        "    cache_dir = '/content/drive/MyDrive/model',\n",
        ")"
      ],
      "metadata": {
        "id": "17Z0BFgRKRh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XchDlSN8KSVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "dIrr8Iy7vMMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    token=auth_token,\n",
        "    cache_dir = '/content/drive/MyDrive/model',\n",
        "    torch_dtype=torch.float16,\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2},\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map = \"auto\",\n",
        "    quantization_config=quantization_config\n",
        ")"
      ],
      "metadata": {
        "id": "Of_DN4PVLIo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1"
      ],
      "metadata": {
        "id": "8sOvT0Jo4XRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text"
      ],
      "metadata": {
        "id": "U_4EqQVlvPZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdfs(pdf_paths):\n",
        "    texts = []\n",
        "    for path in pdf_paths:\n",
        "        text = extract_text(path)\n",
        "        texts.append(text)\n",
        "    return texts"
      ],
      "metadata": {
        "id": "mAjPgvEEJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "pdf_path = \"/content/drive/MyDrive/data\""
      ],
      "metadata": {
        "id": "xm28OkPJJv_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_paths = glob.glob(pdf_path+\"/*.pdf\")"
      ],
      "metadata": {
        "id": "mQuq6l08Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = extract_text_from_pdfs(pdf_paths)"
      ],
      "metadata": {
        "id": "d6Pe3Vb6Jv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove header/footer artifacts\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Replace multiple whitespaces with single space\n",
        "    text = re.sub(r'(\\n){2,}', '\\n', text)  # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
        "\n",
        "    # Remove common but unnecessary items like references or excess newlines\n",
        "    text = text.replace('\\n', ' ')  # Replace new lines with space to maintain continuity\n",
        "    return text"
      ],
      "metadata": {
        "id": "F57GelOmJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "afIzXLmeJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size = 512, overlap = 50):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  chunks = []\n",
        "  for i in range(0, len(tokens), chunk_size - overlap):\n",
        "    chunk = tokens[i:i + chunk_size]\n",
        "    chunks.append(tokenizer.convert_tokens_to_string(chunk))\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "ho8tgoeSJv_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenize function\n",
        "def tokenize_function(examples):\n",
        "  all_chunks = []\n",
        "  for example in examples['text']:\n",
        "    chunks = chunk_text(example)\n",
        "    for chunk in chunks:\n",
        "      tokenized_chunk = tokenizer(chunk, padding=\"max_length\", truncation=True, max_length=512)\n",
        "      all_chunks.append(tokenized_chunk)\n",
        "\n",
        "  # Transform list of tokenized chunks into a dictionary of lists\n",
        "  batch = {key: [] for key in all_chunks[0].keys()}\n",
        "  for chunk in all_chunks:\n",
        "    for key, value in chunk.items():\n",
        "      batch[key].append(value)\n",
        "  return batch"
      ],
      "metadata": {
        "id": "9gTd1SEWJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset from the extracted texts\n",
        "texts_dataset = Dataset.from_dict({\"text\": texts})\n",
        "tokenized_dataset = texts_dataset.map(tokenize_function, batched = True, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "UrD84o0CJv_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add labels (in causal language modeling, labels are the same as input_ids)\n",
        "def add_labels(example):\n",
        "    example['labels'] = example['input_ids'].copy()\n",
        "    return example\n",
        "\n",
        "tokenized_dataset = tokenized_dataset.map(add_labels, batched=False)"
      ],
      "metadata": {
        "id": "hrBSJRjFjVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 fine tuning"
      ],
      "metadata": {
        "id": "3OEVuseHbV51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM"
      ],
      "metadata": {
        "id": "UlqhfdGqvUMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SOURCE https://github.com/artidoro/qlora/blob/main/qlora.py\n",
        "def find_all_linear_names(model):\n",
        "    cls = bitsandbytes.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "7kKIVFVlM34q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modules = find_all_linear_names(model)"
      ],
      "metadata": {
        "id": "DdsNP3a1M9lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LoRA configuration\n",
        "lora_config_phase1 = LoraConfig(\n",
        "    r = 16,  # rank of the low-rank approximation\n",
        "    lora_alpha = 64,  # scaling factor\n",
        "    target_modules = modules,  # target specific modules\n",
        "    lora_dropout = 0.1,  # dropout rate\n",
        "    bias = \"none\",  # whether to train biases\n",
        "    task_type = \"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "3yfTeIRSLaN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "Is70c-0i75Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "NJG6ZuWoPbZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config_phase1)"
      ],
      "metadata": {
        "id": "3FhaOWFX3szL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "L19JtlHSvVr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    warmup_steps = 2,\n",
        "    max_steps = 15,\n",
        "    learning_rate = 2e-4,\n",
        "    fp16 = True,\n",
        "    logging_steps = 1,\n",
        "    output_dir = \"outputs\",\n",
        "    optim = \"paged_adamw_8bit\",\n",
        ")"
      ],
      "metadata": {
        "id": "Oig7kzU0MjxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset,\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "SHLTRd4R9Fvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "IFFcMh0r_70N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "id": "Tx0qDgKYNDjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_result.metrics\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "54xcB0PqQWNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/saved_models/phase1'\n",
        "trainer.model.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "zTm_lqCPQWLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6cUtFoKTQWJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    output_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model = model.merge_and_unload()"
      ],
      "metadata": {
        "id": "dKThePAYQWHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_merged_dir = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1\"\n",
        "model.save_pretrained(\n",
        "    output_merged_dir,\n",
        "    safe_serialization=True\n",
        ")"
      ],
      "metadata": {
        "id": "dBDloinyRLPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.save_pretrained(output_merged_dir)"
      ],
      "metadata": {
        "id": "aZyGof3KRWAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(',\\n'.join(os.listdir(\"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1\")))"
      ],
      "metadata": {
        "id": "uZ0WluUZVqdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 1 Testing"
      ],
      "metadata": {
        "id": "bJiGRfvZR1Tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68o7PJWvTwiG"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown"
      ],
      "metadata": {
        "id": "_l-S0pJmKt6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the responses of the chatbot\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information to the user's queries.\n",
        "Please ensure that your responses are succinct, respectful, and factual. If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\"\"\""
      ],
      "metadata": {
        "id": "VqiE1U5be8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the concept of plaque-years in the context of LDL cholesterol and cardiovascular health?\"\n",
        "\n",
        "prompt_with_system_prompt = f\"{system_prompt}\\nUser: {prompt} Assistant: \"  # Add the system prompt to the beginning of the conversation\n",
        "\n",
        "inputs = tokenizer(prompt_with_system_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    #max_length=150,\n",
        "    temperature=0.5,\n",
        "    top_p=0.75\n",
        ")\n",
        "\n",
        "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "display(Markdown(f\"\\n'''\\n{response_text.split('Assistant: ')[-1].strip()}\\n'''\\n\"))"
      ],
      "metadata": {
        "id": "ZFx43fQ8uoG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}