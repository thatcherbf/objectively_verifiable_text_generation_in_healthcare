{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial installs"
      ],
      "metadata": {
        "id": "KzlAGGJEe3li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG3xcE3SMUO_"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7l6Pm3yMVcN"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF --quiet"
      ],
      "metadata": {
        "id": "I-59y8xFohIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "VXLYq2lcseA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Pmg3K6IjVyIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X38U9DPYMa20"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IqjjIaVMcB0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVH9ojzyxc-U"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "import gc\n",
        "import os\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "YS0CjhnJqe-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index\n",
        "import llama_index.readers\n",
        "import llama_index.readers.file\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, download_loader\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "etyBhoLkq5ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace/Drive Interfacing"
      ],
      "metadata": {
        "id": "8B5JK10te9j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "Xhpos-g_w5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsIMFzk6MfAz"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "ENlFRCB5fEEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksmNxIWBMeAs"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Free GPU Memory"
      ],
      "metadata": {
        "id": "4A38W04UTPKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YQvGvA2ITO6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in the Model"
      ],
      "metadata": {
        "id": "XLqgTI98euvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAubK-BuMgjd"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and final trained model\n",
        "model_directory = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1-v2\"\n",
        "auth_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_directory,\n",
        "    local_files_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "fsjYm73Wx5r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf7TT2EMiAr"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_directory,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2},\n",
        "    local_files_only = True,\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "WK7i9RIjc70d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Initial Classifier"
      ],
      "metadata": {
        "id": "TDn6pvgCeB5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "stage1_classifier = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/classifiers/v1')"
      ],
      "metadata": {
        "id": "NyY81_8IeIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_classifier.to(device)\n",
        "stage1_classifier.eval()\n",
        "print(\"Classifer loaded successfully.\")"
      ],
      "metadata": {
        "id": "iN1ESTjCeJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and input formatting\n",
        "def encode_data(tokenizer, texts, max_len=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens = True,\n",
        "            max_length = max_len,\n",
        "            truncation = True,\n",
        "            padding = 'max_length',\n",
        "            return_attention_mask = True,\n",
        "            return_tensors = 'pt',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim = 0)\n",
        "    attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "97qnmCQYe37X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, tokenizer, text, label_dict):\n",
        "    # Encode the text using the provided tokenizer\n",
        "    input_ids, attention_masks = encode_data(tokenizer, [text])\n",
        "\n",
        "    # Move tensors to the same device as the model\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids = None, attention_mask = attention_masks)\n",
        "\n",
        "    # Extract the predicted index and map to label name\n",
        "    prediction_idx = torch.argmax(outputs.logits, dim = 1).item()\n",
        "    return label_dict[prediction_idx]"
      ],
      "metadata": {
        "id": "R3dFK5EQe6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {0: 'quantitative analysis', 1: 'general information', 2: 'miscellaneous'}"
      ],
      "metadata": {
        "id": "AxOygYBZe-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webpage Layout Interface"
      ],
      "metadata": {
        "id": "SeLOAGlsew-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATIC_DIR = os.path.abspath('/content/interface/static')"
      ],
      "metadata": {
        "id": "UCko-PoyV2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Branches"
      ],
      "metadata": {
        "id": "Y-5AetXurLUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge Branch"
      ],
      "metadata": {
        "id": "47Qtwm_urMdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_system_prompt = \"\"\"[INST] <<SYS>>\n",
        "You are an informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information about Cardiovascular disease and adjacent topics in response to the user's queries.\n",
        "Please ensure that your responses are informative, helpful, direct, dispassionate, and factual. Respond in plain English, and aim for your response to be at least 3 sentences in length.\n",
        "Do not include introductory phrases such as: \"Sure! I can help with that\", \"Of course\", \"I believe\", \"Based on the context\", or any other form of greeting.\n",
        "Do not include summary statements such as \"I hope this helps,\" \"Let me know if you need anything else,\" \"In summary,\" or \"To conclude.\"\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "Directly answer to the User's question ONLY, without preamble or reference to the provided context. Do not impersonate the User and do not include followup questions in your response unless prompted.\n",
        "Do NOT reference the provided context or provided information in your answer as the User is aware of these already.\n",
        "REPEAT: Do NOT reference the context or include phrases like \"based on the context\" in your response.\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\\nAssistant: [/INST]\")"
      ],
      "metadata": {
        "id": "Ht-MqDF2rOAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a HF LLM using the llama index wrapper\n",
        "knowledge_llm = HuggingFaceLLM(\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 512,\n",
        "    generate_kwargs = {\"temperature\": 0.6},\n",
        "    system_prompt = knowledge_system_prompt,\n",
        "    query_wrapper_prompt = query_wrapper_prompt,\n",
        "    model = model,\n",
        "    tokenizer = tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "5BqphHTCsxkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and dl embeddings instance\n",
        "embeddings = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "t0zIaHoqszLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and index multiple PDF documents\n",
        "def load_and_index_documents(directory_path):\n",
        "    loader = PyMuPDFReader()\n",
        "    all_documents = []\n",
        "    for pdf_file in Path(directory_path).rglob('*.pdf'):\n",
        "        documents = loader.load(file_path = pdf_file, metadata = True)\n",
        "        all_documents.extend(documents)\n",
        "\n",
        "    # Create an index with all documents\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        all_documents,\n",
        "        embed_model = embeddings\n",
        "    )\n",
        "\n",
        "    return index"
      ],
      "metadata": {
        "id": "tGOvPbAYs0Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and index documents from a specified directory\n",
        "directory_path = '/content/drive/MyDrive/data/'\n",
        "doc_index = load_and_index_documents(directory_path)\n",
        "\n",
        "def serve_knowledge(prompt):\n",
        "    # Setup index query engine using LLM\n",
        "    query_engine = doc_index.as_query_engine(llm = knowledge_llm)\n",
        "\n",
        "    response = query_engine.query(prompt)\n",
        "    return {\"text\": response.response.strip(), \"line\": None}"
      ],
      "metadata": {
        "id": "ULuHewJns1Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative Branch"
      ],
      "metadata": {
        "id": "of-yr-bIrU1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Calculate Functions"
      ],
      "metadata": {
        "id": "Zqahz6h21SrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# average LDL values for men and women from ages 30 to 80\n",
        "men_mean_LDL = [3.3174731188868307, 3.334143838077217, 3.350898329725846, 3.3677370147998453, 3.3846603163817544, 3.4016686596801553, 3.4187624720403567, 3.435942182955132, 3.45320822407551, 3.4705610292216185, 3.4880010343935868, 3.4955003878975948, 3.54100077579519, 3.555500387897595, 3.611998965606413, 3.655001292991983, 3.6629997414016033, 3.6299999999999994, 3.6829997414016034, 3.6814998707008013, 3.695500387897595, 3.68899922420481, 3.700499094905611, 3.724998707008017, 3.7459994828032066, 3.7070002585983968, 3.7089992242048098, 3.714000517196793, 3.721499870700802, 3.714000517196793, 3.718500129299198, 3.69899922420481, 3.6940005171967933, 3.6870002585983968, 3.6940005171967933, 3.6570002585983965, 3.6470002585983967, 3.6449987070080163, 3.631998965606413, 3.613838970778381, 3.595769775924489, 3.5777909270448665, 3.559901972409642, 3.542102462547594, 3.524391950234856, 3.506769990483682, 3.4892361405312635, 3.471789959828607, 3.454431010029464, 3.437158854979317, 3.41997306070442]\n",
        "women_mean_LDL = [2.861743860038676, 2.8833691285024443, 2.905157812093143, 2.9271111456857857, 2.949230373486938, 2.971516749105227, 2.9939715356223946, 3.016596005664881, 3.03939144147595, 3.062359134988363, 3.085500387897595, 3.0970002585983964, 3.12, 3.162999741401603, 3.1659994828032065, 3.2199999999999998, 3.2514998707008016, 3.3110007757951894, 3.351998965606413, 3.43, 3.497000258598397, 3.5314998707008014, 3.601998965606413, 3.654000517196793, 3.7199999999999998, 3.7695009050943886, 3.7919989656064126, 3.821998965606413, 3.8435014222911814, 3.83899922420481, 3.9010007757951897, 3.9270002585983965, 3.934998707008017, 3.9259994828032063, 3.9440005171967933, 3.9495009050943883, 3.9449987070080166, 3.9380010343935865, 3.9570002585983968, 3.9329997414016034, 3.9529997414016034, 3.9332347426945957, 3.9135685689811224, 3.8940007261362166, 3.8745307225055354, 3.8551580688930076, 3.8358822785485422, 3.8167028671558, 3.797619352820021, 3.7786312560559208, 3.759738099775641]\n",
        "\n",
        "# average SBP values for men and women from ages 30 to 80\n",
        "men_mean_SBP = [127.09770000000009, 127.64270000000009, 128.1877000000001, 128.73270000000008, 129.27770000000007, 129.82270000000005, 130.36770000000004, 130.91270000000003, 131.45770000000002, 132.0027, 132.5477, 132.5595, 133.0417, 133.9244, 133.8602, 134.1759, 134.8107, 135.0133, 136.0058, 136.2948, 136.8521, 137.2601, 137.7067, 138.7961, 139.1646, 139.5142, 140.0917, 140.7533, 141.3936, 142.006, 142.6065, 143.1211, 143.6272, 144.0828, 145.4364, 145.5392, 146.494, 146.6693, 146.9347, 147.3451, 147.8901, 148.43509999999998, 148.98009999999996, 149.52509999999995, 150.07009999999994, 150.61509999999993, 151.16009999999991, 151.7050999999999, 152.2500999999999, 152.79509999999988, 153.34009999999986]\n",
        "women_mean_SBP = [111.82199999999999, 112.72099999999999, 113.61999999999999, 114.51899999999999, 115.41799999999999, 116.317, 117.216, 118.115, 119.014, 119.913, 120.812, 121.5938, 122.2175, 122.8461, 123.4768, 124.808, 125.45, 126.2387, 127.1368, 128.1205, 129.2104, 130.3305, 131.2608, 132.1755, 132.5652, 133.3094, 134.2712, 135.0678, 136.1804, 137.2556, 137.6882, 139.0932, 140.3054, 141.3204, 142.0509, 142.8775, 144.2726, 145.0231, 145.8587, 146.5092, 145.9757, 146.8747, 147.7737, 148.6727, 149.5717, 150.4707, 151.3697, 152.2687, 153.1677, 154.0667, 154.9657]\n",
        "\n",
        "men_Ha = [0.9999, 0.9999, 0.9998, 0.9999, 0.9998, 0.9997, 0.9997, 0.9997, 0.9995, 0.9994, 0.999, 0.9993, 0.9989, 0.9991, 0.9989, 0.9983, 0.9985, 0.9984, 0.998, 0.9975, 0.997, 0.9974, 0.9966, 0.9966, 0.996, 0.9953, 0.9953, 0.9951, 0.9944, 0.9945, 0.9934, 0.9936, 0.9929, 0.9931, 0.9923, 0.9925, 0.9922, 0.9913, 0.9915, 0.9911, 0.9909, 0.99, 0.9899, 0.9898, 0.9891, 0.9881, 0.9879, 0.9873, 0.9858, 0.987, 0.9855]\n",
        "men_Hb = [1, 1, 0.9999, 1, 0.9999, 1, 0.9999, 1, 0.9999, 0.9998, 0.9997, 0.9999, 0.9997, 0.9998, 0.9997, 0.9995, 0.9996, 0.9995, 0.9994, 0.9994, 0.9991, 0.9993, 0.9989, 0.9988, 0.9987, 0.9986, 0.9984, 0.9982, 0.9979, 0.9977, 0.9971, 0.9972, 0.9965, 0.9962, 0.9953, 0.9948, 0.9944, 0.9929, 0.9924, 0.991, 0.9901, 0.9885, 0.986, 0.9848, 0.9835, 0.9817, 0.9798, 0.9763, 0.9748, 0.9717, 0.9662]\n",
        "\n",
        "women_Ha = [1, 1, 1, 1, 1, 1, 1, 0.9999, 0.9999, 0.9999, 0.9998, 0.9999, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9996, 0.9996, 0.9994, 0.9993, 0.9994, 0.9992, 0.9992, 0.9991, 0.9989, 0.9989, 0.9989, 0.9986, 0.9984, 0.9981, 0.9981, 0.9982, 0.9979, 0.998, 0.9975, 0.997, 0.9969, 0.9968, 0.9964, 0.9961, 0.9956, 0.9955, 0.9951, 0.9944, 0.9939, 0.9934, 0.993, 0.99304, 0.9913, 0.9921]\n",
        "women_Hb = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9997, 0.9997, 0.9996, 0.9995, 0.9995, 0.9994, 0.9992, 0.9992, 0.9991, 0.9989, 0.9988, 0.9986, 0.9985, 0.9982, 0.9978, 0.9976, 0.9971, 0.9967, 0.9963, 0.9957, 0.995, 0.9941, 0.993, 0.992, 0.9911, 0.9902, 0.9885, 0.9874, 0.985, 0.9832, 0.9821, 0.9797]\n",
        "\n",
        "Ha_list = [women_Ha, men_Ha]\n",
        "Hb_list = [women_Hb, men_Hb]\n",
        "\n",
        "mean_LDL_list = [women_mean_LDL, men_mean_LDL]\n",
        "mean_SBP_list = [women_mean_SBP, men_mean_SBP]\n",
        "\n",
        "Ha_list = [women_Ha, men_Ha]\n",
        "Hb_list = [women_Hb, men_Hb]\n",
        "\n",
        "mean_LDL_list = [women_mean_LDL, men_mean_LDL]\n",
        "mean_SBP_list = [women_mean_SBP, men_mean_SBP]\n",
        "\n",
        "# average BMI values for men and women\n",
        "avg_bmi = [26.98912, 27.85051]\n",
        "# average HDL values for men and women\n",
        "avg_hdl = [1.598386, 1.283341]\n",
        "\n",
        "def calculate(age, sex, ldl, ldl_rx, ldl_dec,\n",
        "              age_start_rx_ldl, age_stop_rx_ldl, hdl, sbp, sbp_rx, sbp_dec,\n",
        "              age_start_rx_sbp, age_stop_rx_sbp, smoke, fmr_tob, prevalent_diabetes_35,\n",
        "              bmi, fam_hx_chd):\n",
        "\n",
        "    past_a_sums = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
        "\n",
        "    dm = int(prevalent_diabetes_35)\n",
        "\n",
        "    lnRR_a_list = []\n",
        "    lnRR_b_list = []\n",
        "\n",
        "    lnRR_a = [0 for _ in range(81 - 30)]\n",
        "    lnRR_b = [0 for _ in range(81 - 30)]\n",
        "    a_t = [0 for _ in range(81 - 30)]\n",
        "    b_t = [0 for _ in range(81 - 30)]\n",
        "    c_t = [0 for _ in range(81 - 30)]\n",
        "    d_t = [0 for _ in range(81 - 30)]\n",
        "    e_t = [0 for _ in range(81 - 30)]\n",
        "    f_t = [0 for _ in range(81 - 30)]\n",
        "    m_t = [0 for _ in range(81 - 30)]\n",
        "\n",
        "    LDL_ES_mmol = [0.610294236200972 / 45, 0.696666480552631 / 40]\n",
        "    SBP_ES_mmol = [0.596822386208318 / 450, 0.562215263512178 / 400]\n",
        "\n",
        "    del_LDL = ldl - mean_LDL_list[sex][age-30]\n",
        "    del_SBP = sbp - mean_SBP_list[sex][age-30]\n",
        "\n",
        "    a_beta = [\n",
        "        [\n",
        "            0.66 * 0.85, 0.5,\n",
        "            [0.983679437374588, 0.768813402297644],\n",
        "            [0.467034851783647, 0.417294201034895],\n",
        "            0.800648203696211, 0.502003018790893,\n",
        "            0.00895775920895248, -0.632265086010107,\n",
        "            0.00305563\n",
        "        ],\n",
        "        [\n",
        "            0.66 * 0.85, 0.5,\n",
        "            [0.641703348526382, 0.4042777367951],\n",
        "            [0.396454635449883, 0.351011034242294],\n",
        "            0.690752315157672, 0.555861893739954,\n",
        "            0.0109834603573848, -0.612663093385597,\n",
        "            0.00305563\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    b_beta = [\n",
        "        [\n",
        "            0.00432149552442511, 0.0000196190161699591,\n",
        "            [1.11270781170849, 1.23876285117286],\n",
        "            [0.336045717103265, 0.462419315914295],\n",
        "            0.515257189855812, 0.0179926569543217,\n",
        "            0.0140518092696023, -0.201163272313767\n",
        "        ],\n",
        "        [\n",
        "            0.00866812947141468, 0.000260041603209642,\n",
        "            [1.29162125334208, 0.857058215628963],\n",
        "            [0.453034845319801, 0.382734719097288],\n",
        "            0.592360632788394, 0.0374768858776153,\n",
        "            0.0148344238334535, 0.00425393915728762\n",
        "        ]\n",
        "    ]\n",
        "\n",
        "    for i_a in range(30, 81):\n",
        "        a_sums = []\n",
        "        a0 = del_LDL * a_beta[sex][0] * (i_a - 20)\n",
        "        a1 = -0.12 if ldl_rx == 1 and i_a >= age_start_rx_ldl else 0\n",
        "        a2 = (i_a - age_start_rx_ldl + 1) * ldl_dec if i_a >= age_start_rx_ldl and age_stop_rx_ldl - i_a >= 0 and ldl_rx == 1 else past_a_sums[-1][2]\n",
        "        a3 = a0 + a2\n",
        "        a4 = del_SBP * a_beta[sex][1] * (i_a - 20)\n",
        "        a5 = -0.1 if sbp_rx == 1 and i_a >= age_start_rx_sbp else 0\n",
        "        a6 = (i_a - age_start_rx_sbp + 1) * sbp_dec if i_a >= age_start_rx_sbp and age_stop_rx_sbp - i_a >= 0 and sbp_rx == 1 else past_a_sums[-1][6]\n",
        "        a7 = a4 + a6\n",
        "        a8 = LDL_ES_mmol[sex] * a3 + a1\n",
        "        a9 = SBP_ES_mmol[sex] * a7 + a5\n",
        "\n",
        "        a_sums.extend([a0, a1, a2, a3, a4, a5, a6, a7, a8, a9])\n",
        "        a_sums.extend([\n",
        "            smoke * a_beta[sex][2][dm],\n",
        "            fmr_tob * a_beta[sex][3][dm],\n",
        "            prevalent_diabetes_35 * a_beta[sex][4],\n",
        "            fam_hx_chd * a_beta[sex][5],\n",
        "            (bmi - avg_bmi[sex]) * a_beta[sex][6],\n",
        "            (hdl - avg_hdl[sex]) * a_beta[sex][7],\n",
        "        ])\n",
        "\n",
        "        past_a_sums.append(a_sums)\n",
        "\n",
        "        b_sums = [\n",
        "            del_LDL * b_beta[sex][0],\n",
        "            del_SBP * b_beta[sex][1],\n",
        "            smoke * b_beta[sex][2][dm],\n",
        "            fmr_tob * b_beta[sex][3][dm],\n",
        "            prevalent_diabetes_35 * b_beta[sex][4],\n",
        "            fam_hx_chd * b_beta[sex][5],\n",
        "            (bmi - avg_bmi[sex]) * b_beta[sex][6],\n",
        "            (hdl - avg_hdl[sex]) * b_beta[sex][7]\n",
        "        ]\n",
        "\n",
        "        lnRR_a_list.append(sum(a_sums[8:]))\n",
        "        lnRR_b_list.append(sum(b_sums))\n",
        "\n",
        "    for i in range(81 - 30):\n",
        "        lnRR_a[i] = lnRR_a_list[i]\n",
        "        lnRR_b[i] = lnRR_b_list[i]\n",
        "\n",
        "        if i < age - 30:\n",
        "            a_t[i] = 0\n",
        "            b_t[i] = 0\n",
        "            c_t[i] = 0\n",
        "            d_t[i] = 0\n",
        "            e_t[i] = 0\n",
        "            f_t[i] = 0\n",
        "            m_t[i] = 0\n",
        "\n",
        "        if i == age - 30:\n",
        "            a_t[i] = 1 - (Ha_list[sex][i] ** math.exp(lnRR_a[i]))\n",
        "            b_t[i] = 1 - (Hb_list[sex][i] ** math.exp(lnRR_b[i]))\n",
        "            c_t[i] = b_t[i]\n",
        "            d_t[i] = a_t[i]\n",
        "            e_t[i] = 1 - c_t[i] - d_t[i]\n",
        "            f_t[i] = 0 + d_t[i]\n",
        "            m_t[i] = 0 + c_t[i]\n",
        "        else:\n",
        "            a_t[i] = 1 - (Ha_list[sex][i] ** math.exp(lnRR_a[i]))\n",
        "            b_t[i] = 1 - (Hb_list[sex][i] ** math.exp(lnRR_b[i]))\n",
        "            c_t[i] = e_t[i - 1] * b_t[i]\n",
        "            d_t[i] = e_t[i - 1] * a_t[i]\n",
        "            e_t[i] = e_t[i - 1] - c_t[i] - d_t[i]\n",
        "            f_t[i] = f_t[i - 1] + d_t[i]\n",
        "            m_t[i] = m_t[i - 1] + c_t[i]\n",
        "\n",
        "    return f_t"
      ],
      "metadata": {
        "id": "TO7Y8qNa1VB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_treatment(new_risk, treatments):\n",
        "    global user_risk\n",
        "    actions = f\"lower your LDL by {treatments[0]} mmol/L and lower your SBP by {treatments[1]} mmHg\".replace(' and lower your SBP by 0 mmHg', '').replace('lower your LDL by 0 mmol/L and l', 'l')\n",
        "    return f\"If you {actions}, then based on the risk estimation algorithm you will reduce your risk from {round(user_risk, 1)}% down to {round(new_risk * 100, 1)}%.\""
      ],
      "metadata": {
        "id": "rfOSDExUrZvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_treatment(sex, age, LDL, HDL, SBP, diab, smoke, fmrtob, famhx, bmi, ldl_rx = 0, ldl_dec = 0, sbp_rx = 0, sbp_dec = 0):\n",
        "    result = calculate(age, sex,\n",
        "                       LDL, ldl_rx, ldl_dec, age, 80,\n",
        "                       HDL,\n",
        "                       SBP, sbp_rx, sbp_dec, age, 80,\n",
        "                       smoke, fmrtob, diab, bmi, famhx)\n",
        "    return {\n",
        "        \"text\": format_treatment(result[-1], [abs(ldl_dec), abs(sbp_dec)]),\n",
        "        \"line\": [age] + result\n",
        "    }"
      ],
      "metadata": {
        "id": "FR9dXN1H7uAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def within_tolerance(num1, num2, tolerance):\n",
        "    return abs(num1 - num2) <= tolerance"
      ],
      "metadata": {
        "id": "46LO8Ro9_wEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_recommendation(goal, treatments):\n",
        "    global user_risk\n",
        "    actions = f\"lower your LDL by {abs(treatments[0])} mmol/L and lower your SBP by {abs(treatments[1])} mmHg\".replace(' and lower your SBP by 0 mmHg', '').replace('lower your LDL by 0 mmol/L and l', 'l')\n",
        "    return f\"Based on the risk estimation algorithm, to get your risk down from {round(user_risk, 1)}% to {goal}% or close to it, you should aim to {actions}. However, this is just one possible course of action and you should speak to your doctor about what is healthy for you to do.\""
      ],
      "metadata": {
        "id": "4-iqgJ3m_ygs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendation(sex, age, LDL, HDL, SBP, diab, smoke, fmrtob, famhx, bmi, goal):\n",
        "    values = []\n",
        "\n",
        "    for n_sbp_dec in range(0, 99-math.floor(SBP), -1):\n",
        "        for n_ldl_dec in range(0, -1-math.floor(LDL), -1):\n",
        "            ldl_rx = 0 if n_ldl_dec == 0 else 1\n",
        "            sbp_rx = 0 if n_sbp_dec == 0 else 1\n",
        "\n",
        "            r = calculate(age, sex,\n",
        "                          LDL, ldl_rx, n_ldl_dec, age, 80,\n",
        "                          HDL,\n",
        "                          SBP, sbp_rx, n_sbp_dec, age, 80,\n",
        "                          smoke, fmrtob, diab, bmi, famhx)\n",
        "\n",
        "            values.append([n_ldl_dec, n_sbp_dec, r])\n",
        "\n",
        "    pruned_paths = [e for e in values if within_tolerance(e[-1][-1]*100, goal, 0.4)]\n",
        "\n",
        "    if len(pruned_paths) % 2 == 0:\n",
        "        selected_path = pruned_paths[len(pruned_paths)//2 - 1]\n",
        "    else:\n",
        "        selected_path = pruned_paths[len(pruned_paths)//2]\n",
        "\n",
        "    return {\n",
        "        \"text\": format_recommendation(goal, selected_path[:-1]),\n",
        "        \"line\": selected_path[-1]\n",
        "    }"
      ],
      "metadata": {
        "id": "0uO0NVJs_1-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Agent Functionality"
      ],
      "metadata": {
        "id": "ivq1Axr48Z3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actions = {\n",
        "    \"get_treatment\": get_treatment,\n",
        "    \"get_recommendation\": get_recommendation\n",
        "}"
      ],
      "metadata": {
        "id": "80179J8e8Ydu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Taking inspiration from agents, this branch tries to teach the vrLLM to extract\n",
        "relevant pieces of the query and format it into a JSON object to pass into the\n",
        "calculator.\n",
        "'''\n",
        "\n",
        "agent_template_start = '''<s>[INST] <<SYS>>\n",
        "Assistant is an expert JSON builder designed to assist with a very specific task with a specific output format.\n",
        "\n",
        "Assistant is able to trigger actions for User by only responding with JSON strings that contain \"action\" and \"action_input\" parameters.\n",
        "\n",
        "The only actions available to Assistant are:\n",
        "\n",
        "- \"get_treatment\": For when Assistant is asked how changing the User's LDL and SBP values would affect their cardiovascular risk.\n",
        "  - To use the get_treatment tool, Assistant should respond with nothing but the JSON object containing \"action\" and \"action_input\", like so:\n",
        "    {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": 1.5, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "- \"get_recommendation\": For when Assistant is asked to suggest what steps to take to get the User's cardiovascular risk down to a specific level.\n",
        "  - To use the get_recommendation tool, Assistant should respond with nothing but the JSON object containing \"action\" and \"action_input\", like so:\n",
        "    {\"action\": \"get_recommendation\", \"action_input\": {\"goal\": 5.0}}\n",
        "\n",
        "Here are some previous conversations between the Assistant and User, where the User's current profile is included in the query:\n",
        "User: What would my risk look like if I lowered my LDL by 1.5?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -1.5, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: What would my risk look like if I decrease my LDL to 1.5?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": 1.5 - ldl, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: How much would my risk lower if I reduced my blood pressure to 120?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 0, \"ldl_dec\": 0, \"sbp_rx\": 1, \"sbp_dec\": 120 - sbp}}\n",
        "User: What would my risk look like if I lowered my SBP by 20 points?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 0, \"ldl_dec\": 0, \"sbp_rx\": 1, \"sbp_dec\": -20}}\n",
        "User: What would my risk look like if I decrease my SBP by 30?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 0, \"ldl_dec\": 0, \"sbp_rx\": 1, \"sbp_dec\": -30}}\n",
        "User: What would my risk look like if I decrease my LDL by 2.0?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -2.0, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: What would my risk look like if I cut my LDL in half?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -ldl/2, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: If my blood pressure decreases to 140, how much would that affect my risk?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 0, \"ldl_dec\": 0, \"sbp_rx\": 1, \"sbp_dec\": 140 - sbp}}\n",
        "User: If I reduce my LDL to 3.5 mmol/L, what would my risk look like?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": 3.5 - ldl, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: If I reduce my LDL by 1.5 points, what would my risk look like?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -1.5, \"sbp_rx\": 0, \"sbp_dec\": 0}}\n",
        "User: What would my risk look like if I lowered my LDL by 1.5 and my SBP by 20 points?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -1.5, \"sbp_rx\": 1, \"sbp_dec\": -20}}\n",
        "User: What would my risk look like if I decrease my SBP by 30 and my LDL by 0.5?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 1, \"ldl_dec\": -0.5, \"sbp_rx\": 1, \"sbp_dec\": -30}}\n",
        "User: If I took medication to lower my blood pressure by 10 points, how would that improve by risk score?\n",
        "Assistant: {\"action\": \"get_treatment\", \"action_input\": {\"ldl_rx\": 0, \"ldl_dec\": 0, \"sbp_rx\": 1, \"sbp_dec\": -10}}\n",
        "User: What should I do to lower my risk to 5%?\n",
        "Assistant: {\"action\": \"get_recommendation\", \"action_input\": {\"goal\": 5.0}}\n",
        "User: How do I reduce my risk by half?\n",
        "Assistant: {\"action\": \"get_recommendation\", \"action_input\": {\"goal\": goal/2}}\n",
        "User: How much would I have to lower my cholesterol or blood pressure to reduce my risk by 2.0%?\n",
        "Assistant: {\"action\": \"get_recommendation\", \"action_input\": {\"goal\": goal - 2.0}}\n",
        "User: How do I cut my risk in half?\n",
        "Assistant: {\"action\": \"get_recommendation\", \"action_input\": {\"goal\": goal/2}}\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "'''\n",
        "agent_template_end = '''\n",
        "[/INST]\n",
        "Assistant:\n",
        "'''"
      ],
      "metadata": {
        "id": "LehpzAni8jvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_command(command):\n",
        "    # Format the prompt with the user's command\n",
        "    prompt = agent_template_start + \"User: \" + command + agent_template_end\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(prompt, return_tensors = \"pt\").to(device)\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 200,\n",
        "    )\n",
        "    response_text = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
        "\n",
        "    response_text = response_text.split('User:')[-1]\n",
        "\n",
        "    # Try to find JSON in the response - sometimes it outputs more than needed\n",
        "    json_match = re.findall(r'\\{.*\\}', response_text)\n",
        "\n",
        "    return json_match[0]"
      ],
      "metadata": {
        "id": "behQdmj78lyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def serve_quantitative(prompt):\n",
        "    result = json.loads(process_command(prompt))\n",
        "\n",
        "    global user_profile\n",
        "    global user_risk\n",
        "\n",
        "    # Some of the possible patterns are based on the current LDL, SBP, or risk.\n",
        "    ldl = user_profile['LDL']\n",
        "    sbp = user_profile['SBP']\n",
        "    goal = user_risk\n",
        "    treatment_line = actions[result[\"action\"]](**user_profile, **result[\"action_input\"])\n",
        "\n",
        "    return treatment_line"
      ],
      "metadata": {
        "id": "mLbt98w48nJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Reflection\n",
        "\n",
        "The self-reflection algorithm is also given access to the CAKE."
      ],
      "metadata": {
        "id": "C9RvB8i7KgEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reflection_system_prompt = \"\"\"[INST] <<SYS>>\n",
        "You are an evaluator designed to assist with a very specific task and respond in a very specific way.\n",
        "You will receive a Prompt and a Provisional Response as an input.\n",
        "\n",
        "You will evaluate the provisional response based on the given prompt.\n",
        "You will determine if the response:\n",
        "1. Accurately reflects the details provided in the prompt.\n",
        "2. Is free from discrepancies or contradictions with the prompt.\n",
        "3. Does not introduce any new information not present in the prompt.\n",
        "4. Does not directly reference the \"provided context\" or use similar phrases.\n",
        "5. Is not too short and includes all important details.\n",
        "\n",
        "If the response meets all of these criteria satisfactorily, you should return the original provisional response unchanged.\n",
        "If the response fails to meet one or more of these criteria, you should generate an improved response using only the information contained in the prompt, or modify the existing reponse.\n",
        "\n",
        "Your output should ONLY be one of the following:\n",
        "1. The original provisional response unchanged.\n",
        "2. The improved response without any preamble, explanations, or additional text.\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "\n",
        "reflection_query_wrapper_prompt = SimpleInputPrompt(\"{query_str}[/INST]\")\n",
        "\n",
        "# Create a HF LLM using the llama index wrapper\n",
        "reflection_llm = HuggingFaceLLM(\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 512,\n",
        "    generate_kwargs = {\n",
        "        \"temperature\": 0.5,\n",
        "        \"top_p\": 0.9,\n",
        "        \"top_k\": 50,\n",
        "    },\n",
        "    system_prompt = reflection_system_prompt,\n",
        "    query_wrapper_prompt = reflection_query_wrapper_prompt,\n",
        "    model = model,\n",
        "    tokenizer = tokenizer\n",
        ")\n",
        "\n",
        "def self_reflect(prompt, response):\n",
        "  # Setup index query engine using LLM\n",
        "  query_engine = doc_index.as_query_engine(llm = reflection_llm)\n",
        "  prompt = f\"Prompt: {prompt}\\nProvisional Response: {response}\"\n",
        "  verdict = query_engine.query(prompt)\n",
        "\n",
        "  return verdict.response.strip()"
      ],
      "metadata": {
        "id": "OS23DcRfK9xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User profile\n",
        "\n",
        "\n",
        "```\n",
        "Global variable\n",
        "```"
      ],
      "metadata": {
        "id": "5hNbAHY55Pv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_matching_fields(target_dict, source_dict):\n",
        "    for key in source_dict:\n",
        "        if key in target_dict:\n",
        "            target_dict[key] = source_dict[key]"
      ],
      "metadata": {
        "id": "7zbGA8Pr5OLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "One field per relevant input box in the profile. Height and weight not included\n",
        "due to BMI being the relevant field for the calculation algorithm.\n",
        "get_treatment and get_recommendation receive additional fields so it's not necessary\n",
        "for user_profile to include things like \"ldl_rx\" etc.\n",
        "'''\n",
        "\n",
        "user_profile = {\n",
        "    'sex': None,\n",
        "    'age': None,\n",
        "    'LDL': None,\n",
        "    'HDL': None,\n",
        "    'SBP': None,\n",
        "    'diab': None,\n",
        "    'smoke': None,\n",
        "    'fmrtob': None,\n",
        "    'famhx': None,\n",
        "    'bmi': None\n",
        "}"
      ],
      "metadata": {
        "id": "2Vyft3qS5Uyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_risk = None"
      ],
      "metadata": {
        "id": "r0veoycG5XHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Stream"
      ],
      "metadata": {
        "id": "3danK7VOUoHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Flask app and the context history\n",
        "app = Flask(\"expert-bot\", static_folder = STATIC_DIR)\n",
        "context_history = []"
      ],
      "metadata": {
        "id": "awVdJmxOuSyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_context():\n",
        "    # Define an initial conversation context.\n",
        "    # The system prompt attempts to counteract the friendliness of the model.\n",
        "    initial_context = [\n",
        "        \"System: Please answer the questions put to you without emotion or greeting. Don't mention the context in your answers either.\"\n",
        "    ]\n",
        "\n",
        "    return initial_context"
      ],
      "metadata": {
        "id": "rCUkBYc2GKfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/\")\n",
        "def home():\n",
        "    # Location of the HTML document for the frontend\n",
        "    html_file_path = '/content/interface/index.html'\n",
        "    with open(html_file_path, 'r') as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    return html_content"
      ],
      "metadata": {
        "id": "f_qMIchoV57j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Interaction"
      ],
      "metadata": {
        "id": "aJ5z6i2d9v8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This is the initial calculation, that the user must complete to interact with\n",
        "the bot logic. This ensures that there is a copy of the user's profile in the\n",
        "active variables so that get_treatment and get_recommendation can use it.\n",
        "'''\n",
        "\n",
        "@app.route(\"/calculate\", methods=[\"POST\"])\n",
        "def calculate_risk():\n",
        "    profile_state = request.get_json()\n",
        "    profile_state = {key: float(value) if '.' in value else int(value) for key, value in profile_state.items()}\n",
        "\n",
        "    global user_profile\n",
        "    update_matching_fields(user_profile, profile_state)\n",
        "\n",
        "    age = profile_state['age']\n",
        "    sex = profile_state['sex']\n",
        "    ldl = profile_state['LDL']\n",
        "    ldl_rx = profile_state['ldl_rx']\n",
        "    ldl_dec = profile_state['ldl_dec']\n",
        "    age_start_rx_ldl = age\n",
        "    age_stop_rx_ldl = 80\n",
        "\n",
        "    sbp = profile_state['SBP']\n",
        "    sbp_rx = profile_state['sbp_rx']\n",
        "    sbp_dec = profile_state['sbp_dec']\n",
        "    age_start_rx_sbp = age\n",
        "    age_stop_rx_sbp = 80\n",
        "\n",
        "    hdl = profile_state['HDL']\n",
        "    bmi = profile_state['bmi']\n",
        "    diab = profile_state['diab']\n",
        "    smoke = profile_state['smoke']\n",
        "    fmr_tob = profile_state['fmrtob']\n",
        "    famhx = profile_state['famhx']\n",
        "\n",
        "    result = calculate(age, sex,\n",
        "                       ldl, ldl_rx, ldl_dec, age_start_rx_ldl, age_stop_rx_ldl,\n",
        "                       hdl,\n",
        "                       sbp, sbp_rx, sbp_dec, age_start_rx_sbp, age_stop_rx_sbp,\n",
        "                       smoke, fmr_tob, diab, bmi, famhx)\n",
        "\n",
        "    output = {}\n",
        "    output['age'] = age\n",
        "    output['data'] = result\n",
        "\n",
        "    global user_risk\n",
        "    user_risk = result[-1] * 100\n",
        "\n",
        "    return jsonify(output)"
      ],
      "metadata": {
        "id": "3oaH_AIl9za8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hub Interaction"
      ],
      "metadata": {
        "id": "tHlLAZ1m9z42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "As a personal preference, I don't like that LLMs have this artefact despite the\n",
        "fine-tuning. I therefore get rid of it if it shows up.\n",
        "'''\n",
        "\n",
        "pattern = r\"^Sure,?.*?that[.!]?\\s\" # Starts with \"Sure\" and matches up to the first exclamation mark or fullstop."
      ],
      "metadata": {
        "id": "1ZOjBrbxF6Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/interact\", methods=[\"POST\"])\n",
        "def interact():\n",
        "    free_gpu_memory()\n",
        "    global context_history\n",
        "    data = request.get_json()\n",
        "    user_input = data['query']\n",
        "\n",
        "    branch = classify(stage1_classifier, classifier_tokenizer, user_input, label_dict)\n",
        "    print(f'branch: {branch}')\n",
        "\n",
        "    if branch == 'general information':\n",
        "      # Append user input to context as needed\n",
        "      context_history.append(f\"User: {user_input}\")\n",
        "\n",
        "      # Knowledge-based questions keep a context for the conversation.\n",
        "      conversation = '\\n'.join(context_history)\n",
        "      response_text = serve_knowledge(conversation)\n",
        "\n",
        "      # Append the formatted response to the context\n",
        "      context_history.append(f\"Assistant: {response_text['text']}\")\n",
        "      response_text['text'] = re.sub(pattern, \"\", response_text['text'].replace('Sure!', 'Sure,'), flags=re.DOTALL).strip()\n",
        "\n",
        "      # Maintain a recent context window to avoid stale conversation artifacts\n",
        "      if len(context_history) > 9:\n",
        "          # keep the last 9 exchanges (4 User/Assistant pairs and the System prompt)\n",
        "          context_history = context_history[0] + context_history[-8:]\n",
        "\n",
        "      # Self reflection\n",
        "      response_text['text'] = self_reflect(user_input, response_text['text'])\n",
        "\n",
        "      return jsonify(response_text)\n",
        "\n",
        "    elif branch == 'quantitative analysis':\n",
        "        response_text = serve_quantitative(user_input)\n",
        "        return jsonify(response_text)\n",
        "\n",
        "    else:\n",
        "        # If an irrelevant question is asked it never reaches the vrLLM.\n",
        "        response_text = {\"text\": \"Sorry, I'm not able to help you with that. Please either rephrase the question or ask a different question.\", \"line\": None}\n",
        "        return jsonify(response_text)"
      ],
      "metadata": {
        "id": "ydF_k8joV-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(7000)\n",
        "\n",
        "    print(f\"Flask app is running at {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(host='0.0.0.0', port=7000)"
      ],
      "metadata": {
        "id": "2v8blKF9WAsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEgRdorAWMo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}