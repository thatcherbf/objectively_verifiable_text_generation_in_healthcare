{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial installs"
      ],
      "metadata": {
        "id": "KzlAGGJEe3li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG3xcE3SMUO_"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7l6Pm3yMVcN"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF --quiet"
      ],
      "metadata": {
        "id": "I-59y8xFohIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "VXLYq2lcseA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Pmg3K6IjVyIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X38U9DPYMa20"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IqjjIaVMcB0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVH9ojzyxc-U"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "YS0CjhnJqe-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index\n",
        "import llama_index.readers\n",
        "import llama_index.readers.file\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, download_loader\n",
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "etyBhoLkq5ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace/Drive interfacing"
      ],
      "metadata": {
        "id": "8B5JK10te9j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "Xhpos-g_w5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsIMFzk6MfAz"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "ENlFRCB5fEEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksmNxIWBMeAs"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Free GPU Memory"
      ],
      "metadata": {
        "id": "4A38W04UTPKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YQvGvA2ITO6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in the model"
      ],
      "metadata": {
        "id": "XLqgTI98euvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAubK-BuMgjd"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and final trained model\n",
        "model_directory = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1-v2\"\n",
        "auth_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_directory,\n",
        "    local_files_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "fsjYm73Wx5r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf7TT2EMiAr"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_directory,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2},\n",
        "    local_files_only = True,\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "WK7i9RIjc70d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load initial classifier"
      ],
      "metadata": {
        "id": "TDn6pvgCeB5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "stage1_classifier = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/classifiers/v1')"
      ],
      "metadata": {
        "id": "NyY81_8IeIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_classifier.to(device)\n",
        "stage1_classifier.eval()\n",
        "print(\"Classifer loaded successfully.\")"
      ],
      "metadata": {
        "id": "iN1ESTjCeJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(tokenizer, texts, max_len=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "97qnmCQYe37X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, tokenizer, text, label_dict):\n",
        "    input_ids, attention_masks = encode_data(tokenizer, [text])\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    prediction_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return label_dict[prediction_idx]"
      ],
      "metadata": {
        "id": "R3dFK5EQe6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {0: 'quantitative analysis', 1: 'general information', 2: 'miscellaneous'}"
      ],
      "metadata": {
        "id": "AxOygYBZe-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webpage layout interface"
      ],
      "metadata": {
        "id": "SeLOAGlsew-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATIC_DIR = os.path.abspath('/content/interface/static')"
      ],
      "metadata": {
        "id": "UCko-PoyV2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Branches"
      ],
      "metadata": {
        "id": "Y-5AetXurLUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Knowledge branch"
      ],
      "metadata": {
        "id": "47Qtwm_urMdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knowledge_system_prompt = \"\"\"[INST] <<SYS>>\n",
        "You are an informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information about Cardiovascular disease and adjacent topics in response to the user's queries.\n",
        "Please ensure that your responses are informative, helpful, direct, dispassionate, and factual. Respond in plain English, and aim for your response to be at least 3 sentences in length.\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "\n",
        "query_wrapper_prompt = SimpleInputPrompt(\"{query_str}\\nAssistant: [/INST]\")"
      ],
      "metadata": {
        "id": "Ht-MqDF2rOAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a HF LLM using the llama index wrapper\n",
        "knowledge_llm = HuggingFaceLLM(\n",
        "    context_window = 4096,\n",
        "    max_new_tokens = 512,\n",
        "    generate_kwargs = {\"temperature\": 0.6},\n",
        "    system_prompt = knowledge_system_prompt,\n",
        "    query_wrapper_prompt = query_wrapper_prompt,\n",
        "    model = model,\n",
        "    tokenizer = tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "5BqphHTCsxkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and dl embeddings instance\n",
        "embeddings = LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
        ")"
      ],
      "metadata": {
        "id": "t0zIaHoqszLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and index multiple PDF documents\n",
        "def load_and_index_documents(directory_path):\n",
        "    loader = PyMuPDFReader()\n",
        "    all_documents = []\n",
        "    for pdf_file in Path(directory_path).rglob('*.pdf'):\n",
        "        documents = loader.load(file_path = pdf_file, metadata = True)\n",
        "        all_documents.extend(documents)\n",
        "\n",
        "    # Create an index with all documents\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        all_documents,\n",
        "        embed_model = embeddings\n",
        "    )\n",
        "    return index"
      ],
      "metadata": {
        "id": "tGOvPbAYs0Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and index documents from a specified directory\n",
        "directory_path = '/content/drive/MyDrive/data/'\n",
        "doc_index = load_and_index_documents(directory_path)\n",
        "\n",
        "def serve_knowledge(prompt):\n",
        "    # Setup index query engine using LLM\n",
        "    query_engine = doc_index.as_query_engine(llm = knowledge_llm)\n",
        "\n",
        "    response = query_engine.query(prompt)\n",
        "    return {\"answer\": response.response.strip()}"
      ],
      "metadata": {
        "id": "ULuHewJns1Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantitative branch"
      ],
      "metadata": {
        "id": "of-yr-bIrU1E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfOSDExUrZvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main stream"
      ],
      "metadata": {
        "id": "3danK7VOUoHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Flask app and the context history\n",
        "app = Flask(\"expert-bot\", static_folder = STATIC_DIR)\n",
        "context_history = []"
      ],
      "metadata": {
        "id": "awVdJmxOuSyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/\")\n",
        "def home():\n",
        "    html_file_path = '/content/interface/index.html'\n",
        "    with open(html_file_path, 'r') as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    return html_content"
      ],
      "metadata": {
        "id": "f_qMIchoV57j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the responses of the chatbot\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information to the user's queries.\n",
        "Please ensure that your responses are succinct, respectful, and factual. Refrain from emoting.\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "Respond to the User's question ONLY. Do not impersonate the User and do not include followup questions in your response unless prompted.\"\"\""
      ],
      "metadata": {
        "id": "k5lY3UISyVNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_history += [system_prompt]"
      ],
      "metadata": {
        "id": "3Fy77rU2yb1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/interact\", methods=[\"POST\"])\n",
        "def interact():\n",
        "    global context_history\n",
        "    data = request.get_json()\n",
        "    user_input = data['query']\n",
        "\n",
        "    branch = classify(stage1_classifier, classifier_tokenizer, user_input, label_dict)\n",
        "\n",
        "    if branch == 'general information':\n",
        "        # Append user input to context as needed\n",
        "        context_history.append(f\"User: {user_input}\")\n",
        "\n",
        "        # Knowledge-based questions keep a context for the conversation.\n",
        "        conversation = '\\n'.join(context_history)\n",
        "        response_text = serve_knowledge(conversation)\n",
        "\n",
        "        # Append the formatted response to the context\n",
        "        context_history.append(f\"Assistant: {response_text['text']}\")\n",
        "\n",
        "        # Maintain a recent context window to avoid stale conversation artifacts\n",
        "        if len(context_history) > 9:\n",
        "            # keep the last 9 exchanges (4 User/Assistant pairs and the System prompt)\n",
        "            context_history = context_history[0] + context_history[-8:]\n",
        "\n",
        "        return jsonify(response_text)\n",
        "\n",
        "    elif branch == 'quantitative analysis':\n",
        "        return jsonify({\"answer\": \"quantitative_placeholder\"})\n",
        "\n",
        "    else:\n",
        "        return jsonify({\"answer\": \"Sorry, I'm not able to help you with that. Please either rephrase the question or ask a different question.\"})"
      ],
      "metadata": {
        "id": "ydF_k8joV-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(7000)\n",
        "\n",
        "    print(f\"Flask app is running at {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(host='0.0.0.0', port=7000)"
      ],
      "metadata": {
        "id": "2v8blKF9WAsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEgRdorAWMo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}