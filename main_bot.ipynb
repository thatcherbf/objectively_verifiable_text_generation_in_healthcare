{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial installs"
      ],
      "metadata": {
        "id": "KzlAGGJEe3li"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG3xcE3SMUO_"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.org/simple/ bitsandbytes --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7l6Pm3yMVcN"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF --quiet"
      ],
      "metadata": {
        "id": "I-59y8xFohIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok --quiet"
      ],
      "metadata": {
        "id": "VXLYq2lcseA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial imports"
      ],
      "metadata": {
        "id": "Pmg3K6IjVyIw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X38U9DPYMa20"
      },
      "outputs": [],
      "source": [
        "import bitsandbytes\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IqjjIaVMcB0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVH9ojzyxc-U"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import gc\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import drive, userdata"
      ],
      "metadata": {
        "id": "YS0CjhnJqe-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts.prompts import SimpleInputPrompt\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding"
      ],
      "metadata": {
        "id": "etyBhoLkq5ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace/Drive interfacing"
      ],
      "metadata": {
        "id": "8B5JK10te9j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "Xhpos-g_w5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsIMFzk6MfAz"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "ENlFRCB5fEEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksmNxIWBMeAs"
      },
      "outputs": [],
      "source": [
        "# Check device availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Free GPU Memory"
      ],
      "metadata": {
        "id": "4A38W04UTPKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def free_gpu_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YQvGvA2ITO6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load in the model"
      ],
      "metadata": {
        "id": "XLqgTI98euvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAubK-BuMgjd"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and final trained model\n",
        "model_directory = \"/content/drive/MyDrive/saved_models/LLama2-7B-chat-PT1-v2\"\n",
        "auth_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_directory,\n",
        "    local_files_only = True\n",
        ")"
      ],
      "metadata": {
        "id": "fsjYm73Wx5r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf7TT2EMiAr"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_directory,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2},\n",
        "    local_files_only = True,\n",
        "    quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(\"Model loaded successfully.\")"
      ],
      "metadata": {
        "id": "WK7i9RIjc70d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load initial classifier"
      ],
      "metadata": {
        "id": "TDn6pvgCeB5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-v1.1')\n",
        "stage1_classifier = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/classifiers/v1')"
      ],
      "metadata": {
        "id": "NyY81_8IeIQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage1_classifier.to(device)\n",
        "stage1_classifier.eval()\n",
        "print(\"Classifer loaded successfully.\")"
      ],
      "metadata": {
        "id": "iN1ESTjCeJrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(tokenizer, texts, max_len=256):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "97qnmCQYe37X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(model, tokenizer, text, label_dict):\n",
        "    input_ids, attention_masks = encode_data(tokenizer, [text])\n",
        "\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "\n",
        "    prediction_idx = torch.argmax(outputs.logits, dim=1).item()\n",
        "    return label_dict[prediction_idx]"
      ],
      "metadata": {
        "id": "R3dFK5EQe6fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {0: 'quantitative analysis', 1: 'general information', 2: 'miscellaneous'}"
      ],
      "metadata": {
        "id": "AxOygYBZe-Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webpage layout interface"
      ],
      "metadata": {
        "id": "SeLOAGlsew-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STATIC_DIR = os.path.abspath('/content/interface/static')"
      ],
      "metadata": {
        "id": "UCko-PoyV2wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main stream"
      ],
      "metadata": {
        "id": "3danK7VOUoHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Flask app and the context history\n",
        "app = Flask(\"expert-bot\", static_folder = STATIC_DIR)\n",
        "context_history = []"
      ],
      "metadata": {
        "id": "awVdJmxOuSyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/\")\n",
        "def home():\n",
        "    html_file_path = '/content/interface/index.html'\n",
        "    with open(html_file_path, 'r') as file:\n",
        "        html_content = file.read()\n",
        "\n",
        "    return html_content"
      ],
      "metadata": {
        "id": "f_qMIchoV57j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a system prompt to guide the responses of the chatbot\n",
        "system_prompt = \"\"\"You are a helpful and informative assistant called \"Assistant\". Your goal is to provide accurate and relevant information to the user's queries.\n",
        "Please ensure that your responses are succinct, respectful, and factual. Refrain from emoting.\n",
        "If you're uncertain about a question, it's better to admit it rather than provide inaccurate information.\n",
        "Respond to the User's question ONLY. Do not impersonate the User and do not include followup questions in your response unless prompted.\"\"\""
      ],
      "metadata": {
        "id": "k5lY3UISyVNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_history += [system_prompt]"
      ],
      "metadata": {
        "id": "3Fy77rU2yb1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route(\"/interact\", methods=[\"POST\"])\n",
        "def interact():\n",
        "    global context_history\n",
        "    data = request.get_json()\n",
        "    user_input = data['query']\n",
        "\n",
        "    branch = classify(stage1_classifier, classifier_tokenizer, user_input, label_dict)\n",
        "\n",
        "    if branch == 'general information':\n",
        "        # Append user input to context as needed\n",
        "        context_history.append(f\"User: {user_input}\")\n",
        "\n",
        "        # Generate the response using the current context, not repeating the user's input\n",
        "        conversation = \"\\n\".join(context_history)\n",
        "\n",
        "        prompt = f\"{conversation}\\n Assistant: \"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=350,\n",
        "            temperature=0.5,\n",
        "            top_p=0.75\n",
        "        )\n",
        "\n",
        "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        assistant_response = response_text.split('Assistant:')[-1].strip()\n",
        "\n",
        "        # Append the formatted response to the context\n",
        "        context_history.append(f\"Assistant: {assistant_response}\")\n",
        "\n",
        "        # Maintain a recent context window to avoid stale conversation artifacts\n",
        "        if len(context_history) > 10:\n",
        "            context_history = context_history[-10:]  # keep the last 10 exchanges\n",
        "\n",
        "        # Only display the Assistant's response to the user, not the entire context\n",
        "        response_to_display = assistant_response\n",
        "\n",
        "        return jsonify({\"answer\": response_to_display})\n",
        "\n",
        "    elif branch == 'quantitative analysis':\n",
        "        return jsonify({\"answer\": \"quantitative_placeholder\"})\n",
        "\n",
        "    else:\n",
        "        return jsonify({\"answer\": \"Sorry, I'm not able to help you with that. Please either rephrase the question or ask a different question.\"})"
      ],
      "metadata": {
        "id": "ydF_k8joV-gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(7000)\n",
        "\n",
        "    print(f\"Flask app is running at {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(host='0.0.0.0', port=7000)"
      ],
      "metadata": {
        "id": "2v8blKF9WAsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XEgRdorAWMo7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}